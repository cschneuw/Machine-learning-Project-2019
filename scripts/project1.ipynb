{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Presentation and Pre-Processing\n",
    "Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "#import os\n",
    "#data_base_path = os.path.join(os.pardir, 'data')\n",
    "#data_folder = 'train.csv'\n",
    "#data_path = os.path.join(data_base_path, data_folder)\n",
    "#y, tX, ids = load_csv_data(data_path)\n",
    "\n",
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple data description (change later)\n",
    "- all variables are floating point, except PRI_jet_num which is integer\n",
    "- variables prefixed with PRI (for PRImitives) are “raw” quantities about the bunch collision as measured by the detector.\n",
    "- variables prefixed with DER (for DERived) are quantities computed from the primitive features, which were selected by the physicists of ATLAS.\n",
    "- it can happen that for some entries some variables are meaningless or cannot be computed; in this case, their value is −999.0, which is outside the normal range of all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('y shape = ' + str(y.shape) + '\\ntX shape =' + str(tX.shape) + '\\nids shape = ' + str(ids.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of the data:\n",
    "- `y` (N) is composed of the labels (-1 or 1) of all the samples.  \n",
    "- `tX` (N x F) is composed of the values of the features (F) for all samples (N)  \n",
    "- `ids` (N) is composed of all the index (100000-349999) of the samples (N)  \n",
    "  \n",
    "Moreover, the number of features is 30 (F=30) and the number of samples is 250'000 (N=250'000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A faire d'après Robin: Dataprocessing: \n",
    "- Truc qui évalue les NA\n",
    "- Fonction qui vire ou non un featues basé sur un seul\n",
    "- Remplace les NA par la moyenne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations_Chris_stand import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Managing the meaningless values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting the meaningless values to \"nan\"\n",
    "tX0 = np.where(tX==-999, np.nan,tX) #nanmin nanstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONNAL: replaces the nan values by medians\n",
    "med_X = np.nanmedian(tX, axis=0)\n",
    "inds = np.where(np.isnan(tX))\n",
    "tX[inds] = np.take(med_X, inds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONNAL : keep only columns that do not have too much missing data\n",
    "tX, rmX = train_data_formatting(tX, degree = 1, cutoff = 0.7, \n",
    "                      imputation = impute_median, interaction = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Managing outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers based on the plots in the Exploratory analysis\n",
    "y, tX = remove_outliers(y, tX, [0, 2, 3, 8, 13, 16, 19, 21, 23, 26],\n",
    "                       [1100, 1000, 1000, 2500, 500, 500, 800, 1800, 800, 600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a sub-sample of training test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONNAL\n",
    "n0 = 20000\n",
    "n = 10000\n",
    "tX = tX[n0:n0+n]\n",
    "y = y[n0:n0+n]\n",
    "ids = ids[n0:n0+n]\n",
    "print(\"tX shape={tXs}\\n y shape={ys}\\n ids shape={ids}\".format(tXs=tX.shape, ys=y.shape, ids=ids.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Managing categorical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONNAL: remove categorical data\n",
    "tX = np.delete(tX, 22, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 3 jet collection for three categories \n",
    "idx0, y_jet0, tX_jet0, idx1, y_jet1, tX_jet1, idx2, y_jet2, tX_jet2 = separate_jet(y, tX)\n",
    "\n",
    "tX_jet0 = np.delete(tX_jet0, 22, axis=1)\n",
    "tX_jet1 = np.delete(tX_jet1, 22, axis=1)\n",
    "tX_jet2 = np.delete(tX_jet2, 22, axis=1)\n",
    "tX_jet0, rmX_jet0 = train_data_formatting(tX_jet0, degree = 1, cutoff = 0.95, \n",
    "                      imputation = impute_median, interaction = False)\n",
    "tX_jet1, rmX_jet1 = train_data_formatting(tX_jet1, degree = 1, cutoff = 0.95, \n",
    "                      imputation = impute_median, interaction = False)\n",
    "tX_jet2, rmX_jet2 = train_data_formatting(tX_jet2, degree = 1, cutoff = 0.95, \n",
    "                      imputation = impute_median, interaction = False)\n",
    "rmX_jet0 = np.append(rmX_jet0, 22)\n",
    "rmX_jet1 = np.append(rmX_jet1, 22)\n",
    "rmX_jet2 = np.append(rmX_jet2, 22)\n",
    "\n",
    "#Standardization of tX\n",
    "tX_jet0 = np.apply_along_axis(standardize, 1, tX_jet0)\n",
    "tX_jet1 = np.apply_along_axis(standardize, 1, tX_jet1)\n",
    "tX_jet2 = np.apply_along_axis(standardize, 1, tX_jet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tX shape=(250000, 30)\n",
      "tX_jet0 shape=(99913, 19)\n",
      "tX_jet1 shape=(77544, 22)\n",
      "tX_jet2+ shape=(72543, 29)\n"
     ]
    }
   ],
   "source": [
    "print(\"tX shape={tXs}\\ntX_jet0 shape={tX0s}\\ntX_jet1 shape={tX1s}\\ntX_jet2+ shape={tX2s}\".format(\n",
    "    tXs=tX.shape, tX0s=tX_jet0.shape, tX1s=tX_jet1.shape, tX2s=tX_jet2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for f in [2, 6, 10, 15, 20, 27]:\n",
    "    plot_feature(ids, tX0, y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Least squares**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(w_ls, loss_ls) = least_squares(y, ntX)\n",
    "\n",
    "print(\"w* ={w}\\n\\nmse={loss}\".format(\n",
    "    w=w_ls, loss=loss_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Least squares with Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define parameters\n",
    "\n",
    "initial_w = np.zeros(ntX.shape[1])\n",
    "max_iters = 100\n",
    "gammas = np.logspace(-6, -1, 50)\n",
    "\n",
    "losses_gd = []\n",
    "ws_gd = []\n",
    "for gamma in gammas:\n",
    "    (w, loss) = least_squares_GD(y, ntX, initial_w, max_iters, gamma)\n",
    "    losses_gd.append(loss)\n",
    "    ws_gd.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the losses per gamma\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogx(gammas, losses_gd)\n",
    "\n",
    "ax.set(xlabel='gamma', ylabel='MSE',\n",
    "       title='Mean square error per choice of learning rate')\n",
    "ax.grid()\n",
    "ax.set_ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmin(losses_gd)\n",
    "\n",
    "loss_gd = losses_gd[idx]\n",
    "w_gd = ws_gd[idx]\n",
    "gamma_gd = gammas[idx]\n",
    "\n",
    "print(\"w* ={w}\\n\\nmse={loss}\\n\\ngamma={gamma}\".format(\n",
    "    w=w_gd, loss=loss_gd, gamma=gamma_gd ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Least squares with Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(ntX.shape[1])\n",
    "max_iters = 100\n",
    "gammas = np.logspace(-6, -1, 50)\n",
    "\n",
    "losses_sgd = [None] * len(gammas)\n",
    "ws_sgd = [None] * len(gammas)\n",
    "for g in range(len(gammas)):\n",
    "    (ws_sgd[g], losses_sgd[g]) = least_squares_SGD(y, ntX, initial_w, max_iters, gammas[g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the losses per gamma used\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogx(gammas, losses_sgd)\n",
    "\n",
    "ax.set(xlabel='gamma', ylabel='MSE',\n",
    "       title='Mean square error per choice of learning rate')\n",
    "ax.grid()\n",
    "ax.set_ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmin(losses_sgd)\n",
    "\n",
    "loss_sgd = losses_sgd[idx]\n",
    "w_sgd = ws_sgd[idx]\n",
    "gamma_sgd = gammas[idx]\n",
    "\n",
    "print(\"w* ={w}\\n\\nmse={loss}\\n\\ngamma={gamma}\".format(\n",
    "    w=ws_sgd[-1], loss=loss_sgd, gamma = gamma_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cross-validation hyperparameter selection*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "degree = 2\n",
    "k_fold = 4\n",
    "lambdas = np.logspace(-15, -1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my = y[0:20000]\n",
    "mtX = ntX[0:20000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_indices = build_k_indices(my, k_fold, seed)\n",
    "\n",
    "rmse_tr_cv = []\n",
    "rmse_te_cv = []\n",
    "\n",
    "for lambda_ in  lambdas:\n",
    "    l_rmse_tr = []\n",
    "    l_rmse_te = []\n",
    "    for k in range(k_fold):\n",
    "        loss_tr, loss_te = cross_validation(my, mtX, k_indices, k, lambda_, degree)\n",
    "        l_rmse_tr.append(np.sqrt(2*loss_tr))\n",
    "        l_rmse_te.append(np.sqrt(2*loss_te))\n",
    "    rmse_tr_cv.append(np.mean(l_rmse_tr))\n",
    "    rmse_te_cv.append(np.mean(l_rmse_te))\n",
    "cross_validation_visualization(lambdas, rmse_tr_cv, rmse_te_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmin(rmse_te_cv)\n",
    "lambda_ri = lambdas[idx]\n",
    "\n",
    "print(\"lambda* ={lambda_}\\n\\nrmse train={rmse_tr}\\n\\nrmse test={rmse_te}\".format(\n",
    "    lambda_=lambda_ri, rmse_tr=rmse_tr_cv[idx], rmse_te=rmse_te_cv[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bias-variance decomposition for complexity determination*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = range(10)\n",
    "ratio_train = 0.7\n",
    "degrees = range(1, 12)\n",
    "\n",
    "rmse_tr_bv = np.empty((len(seeds), len(degrees)))\n",
    "rmse_te_bv = np.empty((len(seeds), len(degrees)))\n",
    "\n",
    "for index_seed, seed in enumerate(seeds):\n",
    "    np.random.seed(seed)\n",
    "        \n",
    "    x_tr, x_te, y_tr, y_te = split_data(mtX, my, ratio_train, seed)        \n",
    "        \n",
    "    mse_tr = []\n",
    "    mse_te = []\n",
    "    \n",
    "    for index_deg, deg in enumerate(degrees): \n",
    "        tx_tr = build_poly(x_tr, deg)\n",
    "        tx_te = build_poly(x_te, deg)\n",
    "            \n",
    "        w_tr, mse_tr = least_squares(y_tr, tx_tr)\n",
    "        mse_te = compute_mse(y_te, tx_te, w_tr)\n",
    "            \n",
    "        rmse_tr_bv[index_seed][index_deg] = np.sqrt(2*np.array(mse_tr))\n",
    "        rmse_te_bv[index_seed][index_deg] = np.sqrt(2*np.array(mse_te))\n",
    "\n",
    "bias_variance_decomposition_visualization(degrees, rmse_tr_bv, rmse_te_bv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = build_poly(mtX, 7)\n",
    "print(poly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = range(10)\n",
    "ratio_train = 0.7\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = split_data(mtX, my, ratio_train, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmin(rmse_te_cv)\n",
    "degree_ri = degrees[idx]\n",
    "\n",
    "print(\"degree* ={dergee}\\n\\nrmse train={rmse_tr}\\n\\nrmse test={rmse_te}\".format(\n",
    "    degree=degree_ri, rmse_tr=rmse_tr_bv[idx], rmse_te=rmse_te_bv[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OTHERS\n",
    "Cross-validation hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "degree_ri = 7\n",
    "k_fold = 4\n",
    "lambdas = np.logspace(-12, -8, 10)\n",
    "degrees = range(10, 13)\n",
    "\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "rmse_tr_ri, rmse_te_ri = cross_validation(y, tX, k_indices, k_fold, degrees, lambdas, ml_function = 'ri', max_iters = 100, gamma = 0.05, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_visualization(degrees, rmse_tr_ri, rmse_te_ri, lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.unravel_index(np.nanargmin(rmse_te_ri), rmse_te_ri.shape)\n",
    "lambda_ri = lambdas[idx[0]]\n",
    "degree_ri = degrees[idx[1]]\n",
    "\n",
    "print(\"lambda*={lambda_}\\n\\ndegree*={degree}\\n\\nrmse train={rmse_tr}\\n\\nrmse test={rmse_te}\".format(\n",
    "    lambda_=lambda_ri, degree=degree_ri, rmse_tr=rmse_tr_ri[idx], rmse_te=rmse_te_ri[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Bias-variance decomposition for complexity determination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idx0, y_jet0, tX_jet0\n",
    "idx1, y_jet1, tX_jet1\n",
    "idx2, y_jet2, tX_jet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarization(y_non_binary):\n",
    "    \"\"\"returns a binary [0,1] list from a binary [-1.0,1.0] list (-1.0->0 and 1.0->1)\"\"\"\n",
    "            \n",
    "    y_binary = np.where(y_non_binary==-1.0, 0, 1)\n",
    "    return y_binary\n",
    "\n",
    "def check_binary(y):\n",
    "    for i in y:\n",
    "        if (i != 0) and (i != 1):\n",
    "            print(\"y value is non-binary !!!\")\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regressionS(idx, y, tX, max_iters, gammas):\n",
    "    \n",
    "    initial_w = np.zeros(tX.shape[1])\n",
    "    losses_lr = np.empty(len(gammas))\n",
    "    ws_lr = np.empty((len(gammas), len(initial_w)))\n",
    "    \n",
    "    for idx, gamma in enumerate(gammas):\n",
    "        (ws_lr[idx, :], losses_lr[idx]) = logistic_regression(y, tX, initial_w, max_iters, gamma)\n",
    "        \n",
    "    #plot the losses per gamma used\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.semilogx(gammas, losses_lr)\n",
    "    ax.set(xlabel='gamma', ylabel='normalized loss', title='Loss per choice of learning rate')\n",
    "    ax.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    idx = np.nanargmin(losses_lr)\n",
    "    w_lr = ws_lr[idx]\n",
    "    gamma_lr = gammas[idx]\n",
    "    print(\"w* ={w}\\n\\nloss ={loss}\\n\\ngamma={gamma}\".format(w=w_lr, loss=losses_lr[idx], gamma = gamma_lr))\n",
    "    \n",
    "    return w_lr, losses_lr, gamma_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wendl\\Documents\\GitHub\\Machine-learning-Project-2019\\scripts\\implementations_Chris_stand.py:229: RuntimeWarning: divide by zero encountered in log\n",
      "  loss -= y[n]*np.log(h)+(1-y[n])*np.log(1-h)\n",
      "C:\\Users\\wendl\\Documents\\GitHub\\Machine-learning-Project-2019\\scripts\\implementations_Chris_stand.py:229: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss -= y[n]*np.log(h)+(1-y[n])*np.log(1-h)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w* =[ -396712.85152998 -1799714.84342767 -1010843.49266186  -153302.03567731\n",
      "   662376.25578737  -153302.10127843   714840.93360389   156250.44004254\n",
      "   633282.81326952  1291571.37747249   283636.42728518   185171.72651376\n",
      "  -340047.62483274   312537.13256488   274980.73100722  -382806.7450548\n",
      "    98712.49857209  -613312.45155087   236681.80989189]\n",
      "\n",
      "loss =0.0\n",
      "\n",
      "gamma=10.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-7405618dacc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mw_lr_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses_lr_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma_lr_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regressionS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_jet0_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_jet0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mw_lr_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses_lr_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma_lr_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regressionS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_jet1_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_jet1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mw_lr_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses_lr_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma_lr_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regressionS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_jet2_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_jet2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgammas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-140527c42e99>\u001b[0m in \u001b[0;36mlogistic_regressionS\u001b[1;34m(idx, y, tX, max_iters, gammas)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgammas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mws_lr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses_lr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m#plot the losses per gamma used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\Machine-learning-Project-2019\\scripts\\implementations_Chris_stand.py\u001b[0m in \u001b[0;36mlogistic_regression\u001b[1;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m         \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_log_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loglikelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# sortir cette ligne du for-----------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\Machine-learning-Project-2019\\scripts\\implementations_Chris_stand.py\u001b[0m in \u001b[0;36mcompute_log_gradient\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;34m\"\"\" Compute the gradient of loss.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m     \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_iters=1000\n",
    "gammas = np.logspace(-10, 1, 20)\n",
    "\n",
    "y_jet0_b = binarization(y_jet0)\n",
    "#check_binary(y_jet0_b)\n",
    "y_jet1_b = binarization(y_jet1)\n",
    "#check_binary(y_jet1_b)\n",
    "y_jet2_b = binarization(y_jet2)\n",
    "#check_binary(y_jet2_b)\n",
    "\n",
    "w_lr_0, losses_lr_0, gamma_lr_0 = logistic_regressionS(idx0, y_jet0_b, tX_jet0, max_iters, gammas)\n",
    "w_lr_1, losses_lr_1, gamma_lr_1 = logistic_regressionS(idx1, y_jet1_b, tX_jet1, max_iters, gammas)\n",
    "w_lr_2, losses_lr_2, gamma_lr_2 = logistic_regressionS(idx2, y_jet2_b, tX_jet2, max_iters, gammas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = sigmoid(((tX_jet0[254]).T).dot(w_lr_0))\n",
    "if (a==0):\n",
    "    print(\"yes\")\n",
    "else:\n",
    "    print(\"no\")\n",
    "np.log(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cross-validation for hyperparameter determination***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_logistic_regressionS(y, tX, k_fold, degrees, gamma_lr, max_iters):\n",
    "    seed = 1\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "    loss_tr_lr, loss_te_lr = cross_validation(y, tX, k_indices, k_fold, degrees, \n",
    "                                          lambdas=[0], ml_function = 'lr', max_iters = 1000, gamma = gamma_lr)\n",
    "    cross_validation_visualization(degrees, loss_tr_lr, loss_te_lr)\n",
    "    \n",
    "    loss_te_lr = np.array(loss_te_lr)\n",
    "    loss_tr_lr = np.array(loss_tr_lr)\n",
    "    #idx = np.nanargmin(loss_te_lr) #indices of the minimum values ignoring NaNs\n",
    "    idx = np.nanargmin(loss_te_lr) \n",
    "    # ---> on ne doit pas prendre l'indice du min loss du TRAINING (et pas testing) ?\n",
    "    degree_lr = degrees[idx]\n",
    "\n",
    "    print(\"degree*={degree}\\nloss train={loss_tr}\\nloss test={loss_te}\\n\".format(\n",
    "        degree=degree_lr, loss_tr=loss_tr_lr.flatten()[idx], loss_te=loss_te_lr.flatten()[idx]))\n",
    "    return degree_lr, loss_te_lr, loss_tr_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = 4\n",
    "degrees = range(0, 9)\n",
    "\n",
    "degree_lr_0, loss_te_lr_0, loss_tr_lr_0 = cross_validation_logistic_regressionS(y_jet0_b, tX_jet0, k_fold, degrees, gamma_lr_0, max_iters)\n",
    "degree_lr_1, loss_te_lr_1, loss_tr_lr_1 = cross_validation_logistic_regressionS(y_jet1_b, tX_jet1, k_fold, degrees, gamma_lr_1, max_iters)\n",
    "degree_lr_2, loss_te_lr_2, loss_tr_lr_2 = cross_validation_logistic_regressionS(y_jet2_b, tX_jet2, k_fold, degrees, gamma_lr_2, max_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tX.shape[1])\n",
    "max_iters = 1000\n",
    "gammas = np.logspace(-10, -1, 20)\n",
    "lambda_rlr =lambda_ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_rlr = np.empty(len(gammas))\n",
    "ws_rlr = np.empty((len(gammas), len(initial_w)))\n",
    "for idx, gamma in enumerate(gammas):\n",
    "    (w, loss) = reg_logistic_regression(y, tX, lambda_rlr, initial_w, max_iters, gamma)\n",
    "    losses_rlr[idx] = loss\n",
    "    ws_rlr[idx, :] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the losses per gamma used\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogx(gammas, losses_rlr)\n",
    "\n",
    "ax.set(xlabel='gamma', ylabel='loglikelihood',\n",
    "       title='Log likelihood per choice of learning rate')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.nanargmin(losses_rlr)\n",
    "\n",
    "w_rlr = ws_rlr[idx]\n",
    "gamma_rlr = gammas[idx]\n",
    "\n",
    "print(\"w* ={w}\\n\\nloglikelihood loss={loss}\\n\\ngamma={gamma}\".format(\n",
    "    w=w_rlr, loss=losses_rlr[idx], gamma = gamma_rlr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***Cross-validation hyperparameter selection***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "degree_rlr = 1\n",
    "k_fold = 4\n",
    "lambdas = np.logspace(-8, -2, 10)\n",
    "degrees = range(3, 12)\n",
    "\n",
    "k_indices = build_k_indices(y, k_fold, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tr_rlr, loss_te_rlr = cross_validation(y, tX, k_indices, k_fold, degrees, \n",
    "                                          lambdas, ml_function = 'rlr', max_iters = 500, gamma = gamma_rlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_visualization(degrees, loss_tr_rlr, loss_te_rlr, lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.unravel_index(np.nanargmin(loss_te_rlr), loss_te_rlr.shape)\n",
    "lambda_rlr = lambdas[idx[0]]\n",
    "degree_rlr = degrees[idx[1]]\n",
    "\n",
    "print(\"lambda* ={lambda_}n\\ndegree*={degree}\\n\\nloglikelihood train={log_tr}\\n\\nloglikelihood test={log_te}\".format(\n",
    "    lambda_=lambda_rlr, degree=degree_rlr, log_tr=loss_tr_rlr[idx], log_te=loss_te_rlr[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
