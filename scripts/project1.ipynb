{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Presentation and Pre-Processing\n",
    "Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "\n",
    "#import os\n",
    "#data_base_path = os.path.join(os.pardir, 'data')\n",
    "#data_folder = 'train.csv'\n",
    "#data_path = os.path.join(data_base_path, data_folder)\n",
    "#y, tX, ids = load_csv_data(data_path)\n",
    "\n",
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple data description (change later)\n",
    "- all variables are floating point, except PRI_jet_num which is integer\n",
    "- variables prefixed with PRI (for PRImitives) are “raw” quantities about the bunch collision as measured by the detector.\n",
    "- variables prefixed with DER (for DERived) are quantities computed from the primitive features, which were selected by the physicists of ATLAS.\n",
    "- it can happen that for some entries some variables are meaningless or cannot be computed; in this case, their value is −999.0, which is outside the normal range of all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('y shape = ' + str(y.shape) + '\\ntX shape =' + str(tX.shape) + '\\nids shape = ' + str(ids.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of the data:\n",
    "- `y` (N) is composed of the labels (-1 or 1) of all the samples.  \n",
    "- `tX` (N x F) is composed of the values of the features (F) for all samples (N)  \n",
    "- `ids` (N) is composed of all the index (100000-349999) of the samples (N)  \n",
    "  \n",
    "Moreover, the number of features is 30 (F=30) and the number of samples is 250'000 (N=250'000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A faire d'après Robin: Dataprocessing: \n",
    "- Truc qui évalue les NA\n",
    "- Fonction qui vire ou non un featues basé sur un seul\n",
    "- Remplace les NA par la moyenne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations_master import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Managing the meaningless values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting the meaningless values to \"nan\"\n",
    "tX0 = np.where(tX==-999, np.nan,tX) #nanmin nanstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONNAL: replaces the nan values by medians\n",
    "med_X = np.nanmedian(tX, axis=0)\n",
    "inds = np.where(np.isnan(tX))\n",
    "tX[inds] = np.take(med_X, inds[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Managing outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers based on the plots in the Exploratory analysis\n",
    "y, tX = remove_outliers(y, tX, [0, 2, 3, 8, 13, 16, 19, 21, 23, 26],\n",
    "                       [1100, 1000, 1000, 2500, 500, 500, 800, 1800, 800, 600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a sub-sample of training test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONNAL\n",
    "n0 = 40000\n",
    "n = 50000\n",
    "tX = tX[n0:n0+n]\n",
    "y = y[n0:n0+n]\n",
    "ids = ids[n0:n0+n]\n",
    "print(\"tX shape={tXs}\\n y shape={ys}\\n ids shape={ids}\".format(tXs=tX.shape, ys=y.shape, ids=ids.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Managing categorical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 3 jet collection for three categories \n",
    "idx0, y_jet0, tX_jet0, idx1, y_jet1, tX_jet1, idx2, y_jet2, tX_jet2 = separate_jet(y, tX)\n",
    "\n",
    "tX_jet0 = np.delete(tX_jet0, 22, axis=1)\n",
    "tX_jet1 = np.delete(tX_jet1, 22, axis=1)\n",
    "tX_jet2 = np.delete(tX_jet2, 22, axis=1)\n",
    "tX_jet0, rmX_jet0 = train_data_formatting(tX_jet0, degree = 1, cutoff = 0.95, \n",
    "                      imputation = impute_median, interaction = False)\n",
    "tX_jet1, rmX_jet1 = train_data_formatting(tX_jet1, degree = 1, cutoff = 0.95, \n",
    "                      imputation = impute_median, interaction = False)\n",
    "tX_jet2, rmX_jet2 = train_data_formatting(tX_jet2, degree = 1, cutoff = 0.95, \n",
    "                      imputation = impute_median, interaction = False)\n",
    "rmX_jet0 = np.append(rmX_jet0, 22)\n",
    "rmX_jet1 = np.append(rmX_jet1, 22)\n",
    "rmX_jet2 = np.append(rmX_jet2, 22)\n",
    "\n",
    "#Standardization of tX\n",
    "tX_jet0 = np.apply_along_axis(standardize, 1, tX_jet0)\n",
    "tX_jet1 = np.apply_along_axis(standardize, 1, tX_jet1)\n",
    "tX_jet2 = np.apply_along_axis(standardize, 1, tX_jet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tX shape={tXs}\\ntX_jet0 shape={tX0s}\\ntX_jet1 shape={tX1s}\\ntX_jet2+ shape={tX2s}\".format(\n",
    "    tXs=tX.shape, tX0s=tX_jet0.shape, tX1s=tX_jet1.shape, tX2s=tX_jet2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrice_show(tX, idx_cat, idx0_sample, idx0_del_feat, idx1_sample, idx1_del_feat, idx2_sample, idx2_del_feat):\n",
    "    \"\"\"returns a big matrices composed of the three jets, each with another color, with deleted data in dark\"\"\"\n",
    "    mat = np.zeros((3*len(tX), len(tX[0])-1)) # 0 if deleted sample or feature\n",
    "    l = len(tX)\n",
    "    \n",
    "    for s in range(len(mat)):\n",
    "        if s<l: #jet0\n",
    "            if np.any(idx0_sample[0]==s):\n",
    "                for v in range(len(mat[s])):\n",
    "                    if (v!=idx_cat) :\n",
    "                        if np.any(idx0_del_feat==v):\n",
    "                            mat[s,v] = 0\n",
    "                        else:\n",
    "                            mat[s,v] = 1 \n",
    "        elif s<l*2:\n",
    "            if np.any(idx1_sample[0]==s-l):\n",
    "                for v in range(len(mat[s])):\n",
    "                    if (v!=idx_cat) :\n",
    "                        if  np.any(idx1_del_feat==v):\n",
    "                            mat[s,v] = 0\n",
    "                        else:\n",
    "                            mat[s,v] = 2\n",
    "        else:\n",
    "            if np.any(idx2_sample[0]==s-2*l):\n",
    "                for v in range(len(mat[s])):\n",
    "                    if (v!=idx_cat) :\n",
    "                        if np.any(idx1_del_feat==v):\n",
    "                            mat[s,v] = 0\n",
    "                        else:\n",
    "                            mat[s,v] = 3\n",
    "    return mat\n",
    "\n",
    "\n",
    "def plot_data(jet):\n",
    "    \"\"\"show a representation of the data divided in jets with deleted samples and features\"\"\"\n",
    "    fig, ax = plt.subplots(nrows=1, figsize=(10, 10)) # figsize=(6, 5.4)\n",
    "    im = ax.imshow(jet, aspect='auto')\n",
    "    #ax.set_title('tX matrix')\n",
    "    ax.set_xlabel('Features')\n",
    "    ax.set_ylabel('Samples')\n",
    "    #ax.set_yticklabels([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_cat = 22\n",
    "idx0_sample = idx0\n",
    "idx0_del_feat = rmX_jet0\n",
    "idx1_sample = idx1\n",
    "idx1_del_feat = rmX_jet1\n",
    "idx2_sample = idx2\n",
    "idx2_del_feat = rmX_jet2\n",
    "\n",
    "mat = matrice_show(tX, idx_cat, idx0_sample, idx0_del_feat, idx1_sample, idx1_del_feat, idx2_sample, idx2_del_feat)\n",
    "plot_data(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for f in [2, 6, 10, 15, 20, 27]:\n",
    "    plot_feature(ids, tX0, y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Least squares**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(w_ls, loss_ls) = least_squares(y, ntX)\n",
    "\n",
    "print(\"w* ={w}\\n\\nmse={loss}\".format(\n",
    "    w=w_ls, loss=loss_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Least squares with Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define parameters\n",
    "\n",
    "initial_w = np.zeros(ntX.shape[1])\n",
    "max_iters = 100\n",
    "gammas = np.logspace(-6, -1, 50)\n",
    "\n",
    "losses_gd = []\n",
    "ws_gd = []\n",
    "for gamma in gammas:\n",
    "    (w, loss) = least_squares_GD(y, ntX, initial_w, max_iters, gamma)\n",
    "    losses_gd.append(loss)\n",
    "    ws_gd.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the losses per gamma\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogx(gammas, losses_gd)\n",
    "\n",
    "ax.set(xlabel='gamma', ylabel='MSE',\n",
    "       title='Mean square error per choice of learning rate')\n",
    "ax.grid()\n",
    "ax.set_ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmin(losses_gd)\n",
    "\n",
    "loss_gd = losses_gd[idx]\n",
    "w_gd = ws_gd[idx]\n",
    "gamma_gd = gammas[idx]\n",
    "\n",
    "print(\"w* ={w}\\n\\nmse={loss}\\n\\ngamma={gamma}\".format(\n",
    "    w=w_gd, loss=loss_gd, gamma=gamma_gd ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Least squares with Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(ntX.shape[1])\n",
    "max_iters = 100\n",
    "gammas = np.logspace(-6, -1, 50)\n",
    "\n",
    "losses_sgd = [None] * len(gammas)\n",
    "ws_sgd = [None] * len(gammas)\n",
    "for g in range(len(gammas)):\n",
    "    (ws_sgd[g], losses_sgd[g]) = least_squares_SGD(y, ntX, initial_w, max_iters, gammas[g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the losses per gamma used\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogx(gammas, losses_sgd)\n",
    "\n",
    "ax.set(xlabel='gamma', ylabel='MSE',\n",
    "       title='Mean square error per choice of learning rate')\n",
    "ax.grid()\n",
    "ax.set_ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmin(losses_sgd)\n",
    "\n",
    "loss_sgd = losses_sgd[idx]\n",
    "w_sgd = ws_sgd[idx]\n",
    "gamma_sgd = gammas[idx]\n",
    "\n",
    "print(\"w* ={w}\\n\\nmse={loss}\\n\\ngamma={gamma}\".format(\n",
    "    w=ws_sgd[-1], loss=loss_sgd, gamma = gamma_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cross-validation hyperparameter selection*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "degree = 2\n",
    "k_fold = 4\n",
    "lambdas = np.logspace(-15, -1, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my = y[0:20000]\n",
    "mtX = ntX[0:20000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_indices = build_k_indices(my, k_fold, seed)\n",
    "\n",
    "rmse_tr_cv = []\n",
    "rmse_te_cv = []\n",
    "\n",
    "for lambda_ in  lambdas:\n",
    "    l_rmse_tr = []\n",
    "    l_rmse_te = []\n",
    "    for k in range(k_fold):\n",
    "        loss_tr, loss_te = cross_validation(my, mtX, k_indices, k, lambda_, degree)\n",
    "        l_rmse_tr.append(np.sqrt(2*loss_tr))\n",
    "        l_rmse_te.append(np.sqrt(2*loss_te))\n",
    "    rmse_tr_cv.append(np.mean(l_rmse_tr))\n",
    "    rmse_te_cv.append(np.mean(l_rmse_te))\n",
    "cross_validation_visualization(lambdas, rmse_tr_cv, rmse_te_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmin(rmse_te_cv)\n",
    "lambda_ri = lambdas[idx]\n",
    "\n",
    "print(\"lambda* ={lambda_}\\n\\nrmse train={rmse_tr}\\n\\nrmse test={rmse_te}\".format(\n",
    "    lambda_=lambda_ri, rmse_tr=rmse_tr_cv[idx], rmse_te=rmse_te_cv[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bias-variance decomposition for complexity determination*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = range(10)\n",
    "ratio_train = 0.7\n",
    "degrees = range(1, 12)\n",
    "\n",
    "rmse_tr_bv = np.empty((len(seeds), len(degrees)))\n",
    "rmse_te_bv = np.empty((len(seeds), len(degrees)))\n",
    "\n",
    "for index_seed, seed in enumerate(seeds):\n",
    "    np.random.seed(seed)\n",
    "        \n",
    "    x_tr, x_te, y_tr, y_te = split_data(mtX, my, ratio_train, seed)        \n",
    "        \n",
    "    mse_tr = []\n",
    "    mse_te = []\n",
    "    \n",
    "    for index_deg, deg in enumerate(degrees): \n",
    "        tx_tr = build_poly(x_tr, deg)\n",
    "        tx_te = build_poly(x_te, deg)\n",
    "            \n",
    "        w_tr, mse_tr = least_squares(y_tr, tx_tr)\n",
    "        mse_te = compute_mse(y_te, tx_te, w_tr)\n",
    "            \n",
    "        rmse_tr_bv[index_seed][index_deg] = np.sqrt(2*np.array(mse_tr))\n",
    "        rmse_te_bv[index_seed][index_deg] = np.sqrt(2*np.array(mse_te))\n",
    "\n",
    "bias_variance_decomposition_visualization(degrees, rmse_tr_bv, rmse_te_bv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = build_poly(mtX, 7)\n",
    "print(poly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = range(10)\n",
    "ratio_train = 0.7\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = split_data(mtX, my, ratio_train, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmin(rmse_te_cv)\n",
    "degree_ri = degrees[idx]\n",
    "\n",
    "print(\"degree* ={dergee}\\n\\nrmse train={rmse_tr}\\n\\nrmse test={rmse_te}\".format(\n",
    "    degree=degree_ri, rmse_tr=rmse_tr_bv[idx], rmse_te=rmse_te_bv[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OTHERS\n",
    "Cross-validation hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "degree_ri = 7\n",
    "k_fold = 4\n",
    "lambdas = np.logspace(-12, -8, 10)\n",
    "degrees = range(10, 13)\n",
    "\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "rmse_tr_ri, rmse_te_ri = cross_validation(y, tX, k_indices, k_fold, degrees, lambdas, ml_function = 'ri', max_iters = 100, gamma = 0.05, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_visualization(degrees, rmse_tr_ri, rmse_te_ri, lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.unravel_index(np.nanargmin(rmse_te_ri), rmse_te_ri.shape)\n",
    "lambda_ri = lambdas[idx[0]]\n",
    "degree_ri = degrees[idx[1]]\n",
    "\n",
    "print(\"lambda*={lambda_}\\n\\ndegree*={degree}\\n\\nrmse train={rmse_tr}\\n\\nrmse test={rmse_te}\".format(\n",
    "    lambda_=lambda_ri, degree=degree_ri, rmse_tr=rmse_tr_ri[idx], rmse_te=rmse_te_ri[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Bias-variance decomposition for complexity determination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "idx0, y_jet0, tX_jet0\n",
    "idx1, y_jet1, tX_jet1\n",
    "idx2, y_jet2, tX_jet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarization(y_non_binary):\n",
    "    \"\"\"returns a binary [0,1] list from a binary [-1.0,1.0] list (-1.0->0 and 1.0->1)\"\"\"\n",
    "            \n",
    "    y_binary = np.where(y_non_binary==-1.0, 0, 1)\n",
    "    return y_binary\n",
    "\n",
    "def check_binary(y):\n",
    "    for i in y:\n",
    "        if (i != 0) and (i != 1):\n",
    "            print(\"y value is non-binary !!!\")\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regressionS(idx, y, tX, max_iters, gammas):\n",
    "    \n",
    "    initial_w = np.zeros(tX.shape[1])\n",
    "    losses_lr = np.empty(len(gammas))\n",
    "    ws_lr = np.empty((len(gammas), len(initial_w)))\n",
    "    \n",
    "    for idx, gamma in enumerate(gammas):\n",
    "        (ws_lr[idx, :], losses_lr[idx]) = logistic_regression(y, tX, initial_w, max_iters, gamma)\n",
    "        \n",
    "    #plot the losses per gamma used\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.semilogx(gammas, losses_lr)\n",
    "    ax.set(xlabel='gamma', ylabel='normalized loss', title='Loss per choice of learning rate')\n",
    "    ax.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    idx = np.nanargmin(losses_lr)\n",
    "    w_lr = ws_lr[idx]\n",
    "    gamma_lr = gammas[idx]\n",
    "    print(\"w* ={w}\\n\\nloss ={loss}\\n\\ngamma={gamma}\".format(w=w_lr, loss=losses_lr[idx], gamma = gamma_lr))\n",
    "    \n",
    "    return w_lr, losses_lr, gamma_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters=1000\n",
    "gammas = np.logspace(-10, 1, 20)\n",
    "\n",
    "y_jet0_b = binarization(y_jet0)\n",
    "#check_binary(y_jet0_b)\n",
    "y_jet1_b = binarization(y_jet1)\n",
    "#check_binary(y_jet1_b)\n",
    "y_jet2_b = binarization(y_jet2)\n",
    "#check_binary(y_jet2_b)\n",
    "\n",
    "w_lr_0, losses_lr_0, gamma_lr_0 = logistic_regressionS(idx0, y_jet0_b, tX_jet0, max_iters, gammas)\n",
    "w_lr_1, losses_lr_1, gamma_lr_1 = logistic_regressionS(idx1, y_jet1_b, tX_jet1, max_iters, gammas)\n",
    "w_lr_2, losses_lr_2, gamma_lr_2 = logistic_regressionS(idx2, y_jet2_b, tX_jet2, max_iters, gammas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cross-validation for hyperparameter determination***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_logistic_regressionS(y, tX, k_fold, degrees, gamma_lr, max_iters):\n",
    "    seed = 1\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "    loss_tr_lr, loss_te_lr = cross_validation(y, tX, k_indices, k_fold, degrees, \n",
    "                                          lambdas=[0], ml_function = 'lr', max_iters = 1000, gamma = gamma_lr)\n",
    "    cross_validation_visualization(degrees, loss_tr_lr, loss_te_lr)\n",
    "    \n",
    "    loss_te_lr = np.array(loss_te_lr)\n",
    "    loss_tr_lr = np.array(loss_tr_lr)\n",
    "    #idx = np.nanargmin(loss_te_lr) #indices of the minimum values ignoring NaNs\n",
    "    idx = np.nanargmin(loss_te_lr) \n",
    "    # ---> on ne doit pas prendre l'indice du min loss du TRAINING (et pas testing) ?\n",
    "    degree_lr = degrees[idx]\n",
    "\n",
    "    print(\"degree*={degree}\\nloss train={loss_tr}\\nloss test={loss_te}\\n\".format(\n",
    "        degree=degree_lr, loss_tr=loss_tr_lr.flatten()[idx], loss_te=loss_te_lr.flatten()[idx]))\n",
    "    return degree_lr, loss_te_lr, loss_tr_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = 4\n",
    "degrees = range(1, 9)\n",
    "\n",
    "degree_lr_0, loss_te_lr_0, loss_tr_lr_0 = cross_validation_logistic_regressionS(y_jet0_b, tX_jet0, k_fold, degrees, gamma_lr_0, max_iters)\n",
    "#degree_lr_1, loss_te_lr_1, loss_tr_lr_1 = cross_validation_logistic_regressionS(y_jet1_b, tX_jet1, k_fold, degrees, gamma_lr_1, max_iters)\n",
    "#degree_lr_2, loss_te_lr_2, loss_tr_lr_2 = cross_validation_logistic_regressionS(y_jet2_b, tX_jet2, k_fold, degrees, gamma_lr_2, max_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tX.shape[1])\n",
    "max_iters = 1000\n",
    "gammas = np.logspace(-10, -1, 20)\n",
    "lambda_rlr =lambda_ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_rlr = np.empty(len(gammas))\n",
    "ws_rlr = np.empty((len(gammas), len(initial_w)))\n",
    "for idx, gamma in enumerate(gammas):\n",
    "    (w, loss) = reg_logistic_regression(y, tX, lambda_rlr, initial_w, max_iters, gamma)\n",
    "    losses_rlr[idx] = loss\n",
    "    ws_rlr[idx, :] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the losses per gamma used\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogx(gammas, losses_rlr)\n",
    "\n",
    "ax.set(xlabel='gamma', ylabel='loglikelihood',\n",
    "       title='Log likelihood per choice of learning rate')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.nanargmin(losses_rlr)\n",
    "\n",
    "w_rlr = ws_rlr[idx]\n",
    "gamma_rlr = gammas[idx]\n",
    "\n",
    "print(\"w* ={w}\\n\\nloglikelihood loss={loss}\\n\\ngamma={gamma}\".format(\n",
    "    w=w_rlr, loss=losses_rlr[idx], gamma = gamma_rlr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "***Cross-validation hyperparameter selection***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "degree_rlr = 1\n",
    "k_fold = 4\n",
    "lambdas = np.logspace(-8, -2, 10)\n",
    "degrees = range(3, 12)\n",
    "\n",
    "k_indices = build_k_indices(y, k_fold, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tr_rlr, loss_te_rlr = cross_validation(y, tX, k_indices, k_fold, degrees, \n",
    "                                          lambdas, ml_function = 'rlr', max_iters = 500, gamma = gamma_rlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_visualization(degrees, loss_tr_rlr, loss_te_rlr, lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.unravel_index(np.nanargmin(loss_te_rlr), loss_te_rlr.shape)\n",
    "lambda_rlr = lambdas[idx[0]]\n",
    "degree_rlr = degrees[idx[1]]\n",
    "\n",
    "print(\"lambda* ={lambda_}n\\ndegree*={degree}\\n\\nloglikelihood train={log_tr}\\n\\nloglikelihood test={log_te}\".format(\n",
    "    lambda_=lambda_rlr, degree=degree_rlr, log_tr=loss_tr_rlr[idx], log_te=loss_te_rlr[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
