{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple data description \n",
    "- all variables are floating point, except PRI_jet_num which is integer\n",
    "- variables prefixed with PRI (for PRImitives) are “raw” quantities about the bunch collision as measured by the detector.\n",
    "- variables prefixed with DER (for DERived) are quantities computed from the primitive features, which were selected by the physicists of ATLAS.\n",
    "- it can happen that for some entries some variables are meaningless or cannot be computed; in this case, their value is −999.0, which is outside the normal range of all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('y shape = ' + str(y.shape) + '\\ntX shape =' + str(tX.shape) + '\\nids shape = ' + str(ids.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of the data:\n",
    "- `y` (N) is composed of the labels (-1 or 1) of all the samples.  \n",
    "- `tX` (N x F) is composed of the values of the features (F) for all samples (N)  \n",
    "- `ids` (N) is composed of all the index (100000-349999) of the samples (N)  \n",
    "  \n",
    "Moreover, the number of features is 30 (F=30) and the number of samples is 250'000 (N=250'000). Non recorded data has value `-999`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tX0 = np.where(tX==-999, np.nan,tX)\n",
    "\n",
    "for f in [2, 6, 10, 15, 20, 27]:\n",
    "    plot_feature(ids, tX0, y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then with this data, we separate the data points based on the categorical data: jet. We keep three matrices with `jet = 0`, `jet = 1` and `jet > 2`. We will futher train separately our models on each of these sub-matrices and have separated, learning rates, penalities or polynomial degrees for each. \n",
    "\n",
    "We still keep the whole data matrix to do the same treatments and generate whole weight matrices as a comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.choice(tX.shape[0], 1500, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y[indices,]\n",
    "tX = tX[indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx0, y_jet0, tX_jet0, idx1, y_jet1, tX_jet1, idx2, y_jet2, tX_jet2 = separate_jet(y, tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each dataset, we clean them by setting all missing values `-999` to `NaN`, then when for a feature the ratio of missing values reaches a threshold, we remove the whole feature from the dataset. The remaining missing data are replaced by the median value in the column (we also tried to replace the missing data by the mean value of the feature or a gaussian distribution. Then we standarize the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingness_cutoff = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX, rmX = missingness_filter(tX, missingness_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX, mtX = impute_median_train(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_jet0, rmX_jet0 = missingness_filter(tX_jet0, missingness_cutoff)\n",
    "tX_jet1, rmX_jet1 = missingness_filter(tX_jet1, missingness_cutoff)\n",
    "tX_jet2, rmX_jet2 = missingness_filter(tX_jet2, missingness_cutoff)\n",
    "tX_jet0, median_jet0 = impute_median_train(tX_jet0)\n",
    "tX_jet1, median_jet1 = impute_median_train(tX_jet1)\n",
    "tX_jet2, median_jet2 = impute_median_train(tX_jet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tX shape={tXs}\\ntX_jet0 shape={tX0s}\\ntX_jet1 shape={tX1s}\\ntX_jet2+ shape={tX2s}\".format(\n",
    "    tXs=tX.shape, tX0s=tX_jet0.shape, tX1s=tX_jet1.shape, tX2s=tX_jet2.shape))\n",
    "\n",
    "print(\"\\nremoved columns for :\\ntX={rmX}\\ntX_jet0={rmX0}\\ntX_jet1={rmX1}\\ntX_jet2+={rmX2}\".format(\n",
    "    rmX=rmX, rmX0=rmX_jet0, rmX1=rmX_jet1, rmX2=rmX_jet2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX1,_,_ = standardize_train(tX.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(w_ls, loss_ls) = least_squares(y, tX1)\n",
    "\n",
    "print(\"w* ={w}\\n\\nmse={loss}\".format(w=w_ls, loss=loss_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jet sub-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for index_jet, (yi, txi) in enumerate(zip([y_jet0, y_jet1, y_jet2], [tX_jet0, tX_jet1, tX_jet2])):\n",
    "    tx1, _, _ = standardize_train(txi.copy())\n",
    "    (w_ls, loss_ls) = least_squares(yi, tx1)\n",
    "    print(\"w_jet{jet}* = {w}\\nmse = {loss}\\n\\n\".format(\n",
    "    jet=index_jet, w=w_ls, loss=loss_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define parameters\n",
    "initial_w = np.zeros(tX.shape[1])\n",
    "max_iters = 100\n",
    "gammas = np.logspace(-6, 0, 20)\n",
    "\n",
    "losses_gd = np.empty(len(gammas))\n",
    "ws_gd = np.empty((len(gammas), len(initial_w)))\n",
    "\n",
    "tX1,_,_ = standardize_train(tX)\n",
    "\n",
    "for idx, gamma in enumerate(gammas):\n",
    "    (w, loss) = least_squares_GD(y, tX1, initial_w, max_iters, gamma)\n",
    "    losses_gd[idx] = loss\n",
    "    ws_gd[idx, :]=w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the losses per gamma\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogx(gammas, losses_gd)\n",
    "\n",
    "ax.set(xlabel='gamma', ylabel='MSE',\n",
    "       title='Mean square error per choice of learning rate')\n",
    "ax.grid()\n",
    "ax.set_ylim([0, 0.8])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.nanargmin(losses_gd)\n",
    "\n",
    "w_gd = ws_gd[idx]\n",
    "gamma_gd = gammas[idx]\n",
    "\n",
    "print(\"w* ={w}\\n\\nmse={loss}\\n\\ngamma={gamma}\".format(\n",
    "    w=w_gd, loss=losses_gd[idx], gamma=gamma_gd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jet sub-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#define parameters\n",
    "max_iters = 100\n",
    "gammas = np.logspace(-6, 0, 10)\n",
    "\n",
    "for index_jet, (yi, txi) in enumerate(zip([y_jet0, y_jet1, y_jet2], [tX_jet0.copy(), tX_jet1.copy(), tX_jet2.copy()])):\n",
    "    tx1, _, _ = standardize_train(txi.copy())\n",
    "    initial_w = np.zeros(tx1.shape[1])\n",
    "    losses_gd = np.empty(len(gammas))\n",
    "    ws_gd = np.empty((len(gammas), len(initial_w)))\n",
    "\n",
    "    for index_g, gamma in enumerate(gammas):\n",
    "        (w, loss) = least_squares_GD(yi, tx1, initial_w, max_iters, gamma)\n",
    "        losses_gd[index_g] = loss\n",
    "        ws_gd[index_g, :]=w\n",
    "        \n",
    "    idx = np.nanargmin(losses_gd)\n",
    "    w_gd = ws_gd[idx]\n",
    "    gamma_gd = gammas[idx]\n",
    "    print(\"w_jet{jet}* = {w}\\nmse = {loss}\\ngamma = {gamma}\".format(\n",
    "    jet=index_jet, w=w_gd, loss=losses_gd[idx], gamma=gamma_gd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tX.shape[1])\n",
    "max_iters = 100\n",
    "gammas = np.logspace(-6, 0, 20)\n",
    "\n",
    "losses_sgd = np.empty(len(gammas))\n",
    "ws_sgd = np.empty((len(gammas), len(initial_w)))\n",
    "\n",
    "tX1, _, _ = standardize_train(tX.copy())\n",
    "\n",
    "for idx, gamma in enumerate(gammas):\n",
    "    (w, loss) = least_squares_SGD(y, tX1, initial_w, max_iters, gamma)\n",
    "    losses_sgd[idx] = loss\n",
    "    ws_sgd[idx,:] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the losses per gamma used\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogx(gammas, losses_sgd)\n",
    "\n",
    "ax.set(xlabel='gamma', ylabel='MSE',\n",
    "       title='Mean square error per choice of learning rate')\n",
    "ax.grid()\n",
    "ax.set_ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.nanargmin(losses_sgd)\n",
    "\n",
    "w_sgd = ws_sgd[idx]\n",
    "gamma_sgd = gammas[idx]\n",
    "\n",
    "print(\"w*={w}\\n\\nmse={loss}\\n\\ngamma={gamma}\".format(\n",
    "    w=w_sgd, loss=losses_sgd[idx], gamma=gamma_sgd ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jet sub-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define parameters\n",
    "max_iters = 100\n",
    "gammas = np.logspace(-6, 0, 10)\n",
    "\n",
    "for index_jet, (yi, txi) in enumerate(zip([y_jet0, y_jet1, y_jet2], [tX_jet0, tX_jet1, tX_jet2])):\n",
    "    tx1, _, _ = standardize_train(txi.copy())\n",
    "    initial_w = np.zeros(tx1.shape[1])\n",
    "    losses_gd = np.empty(len(gammas))\n",
    "    ws_gd = np.empty((len(gammas), len(initial_w)))\n",
    "\n",
    "    for index_g, gamma in enumerate(gammas):\n",
    "        (w, loss) = least_squares_SGD(yi, tx1, initial_w, max_iters, gamma)\n",
    "        losses_gd[index_g] = loss\n",
    "        ws_gd[index_g, :]=w\n",
    "        \n",
    "    idx = np.nanargmin(losses_gd)\n",
    "    w_gd = ws_gd[idx]\n",
    "    gamma_gd = gammas[idx]\n",
    "    print(\"w_jet{jet}* = {w}\\nmse = {loss}\\ngamma = {gamma}\".format(\n",
    "    jet=index_jet, w=w_gd, loss=losses_gd[idx], gamma=gamma_gd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "\n",
    "For ridge regression, we have two parameters to optimize, the lambda (penality) and degree (complexity). To do so, we use a cross validation that optimizes both at the same time. \n",
    "\n",
    "Then a biais and variance decomposition is used to visualize if the methods tend to underfit or overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_interaction = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "degree_ri = 1\n",
    "k_fold = 4\n",
    "lambdas = np.logspace(-20, -8, 13)\n",
    "degrees = range(1, 15)\n",
    "\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "rmse_tr_ri, rmse_te_ri, acc_measures = cross_validation_wAcc(y, tX, k_indices, k_fold, degrees, \n",
    "                                          lambdas , ml_function = 'ri', max_iters = 100, \n",
    "                                          gamma = 0.05, verbose = True, interaction=w_interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_cv_acc(degrees,lambdas,acc_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.unravel_index(np.nanargmax(acc_measures[\"acc_te\"]), acc_measures[\"acc_te\"].shape)\n",
    "lambda_ri = lambdas[idx[0]]\n",
    "degree_ri = degrees[idx[1]]\n",
    "\n",
    "print(\"lambda*={lambda_}\\n\\ndegree*={degree}\\n\\nrmse train={rmse_tr}\\n\\nrmse test={rmse_te}\\n\\nacc train={acc_tr}\\n\\nacc test={acc_te}\".format(\n",
    "    lambda_=lambda_ri, degree=degree_ri, rmse_tr=rmse_tr_ri[idx], rmse_te=rmse_te_ri[idx],acc_tr=acc_measures[\"acc_tr\"][idx], acc_te=acc_measures[\"acc_te\"][idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ri, loss_ri, acc_measures_ri, data_meas_ri = build_final_model(y, tX, degree_ri,\n",
    "                                                      lambda_ri, ml_function = 'ri', interaction = w_interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_meas_ri[\"mean\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc_measures_ri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias-variance decomposition for complexity determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_train = 0.1\n",
    "seeds = range(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_tr_bv = np.empty((len(seeds), len(degrees)))\n",
    "rmse_te_bv = np.empty((len(seeds), len(degrees)))\n",
    "\n",
    "for index_seed, seed in enumerate(seeds):\n",
    "    np.random.seed(seed)\n",
    "    x_tr, x_te, y_tr, y_te = split_data(tX.copy(), y, ratio_train, seed) \n",
    "    \n",
    "    x_tr, x_te = standardize_both(x_tr, x_te)\n",
    "    \n",
    "    for index_deg, deg in enumerate(degrees): \n",
    "        tx_tr = build_poly(x_tr, deg)\n",
    "        tx_te = build_poly(x_te, deg)\n",
    "            \n",
    "        w_tr, mse_tr = ridge_regression(y_tr, tx_tr, lambda_ri)\n",
    "        mse_te = compute_mse(y_te, tx_te, w_tr)\n",
    "            \n",
    "        rmse_tr_bv[index_seed][index_deg] = np.sqrt(2*mse_tr)\n",
    "        rmse_te_bv[index_seed][index_deg] = np.sqrt(2*mse_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_variance_decomposition_visualization(degrees, rmse_tr_bv, rmse_te_bv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plot above, change the value of `degree_ri` manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jet sub-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation for hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "k_fold = 4\n",
    "lambdas = np.logspace(-4, 0, 5)\n",
    "degrees = range(1, 5)\n",
    "\n",
    "k_indices = build_k_indices(y_jet0, k_fold, seed)\n",
    "\n",
    "rmse_tr_ri, rmse_te_ri, acc_measures = cross_validation_wAcc(y_jet0, tX_jet0, k_indices, k_fold, degrees, \n",
    "                                          lambdas , ml_function = 'ri', max_iters = 100, \n",
    "                                          gamma = 0.05, verbose = True, interaction=w_interaction)\n",
    "\n",
    "idx = np.unravel_index(np.nanargmax(acc_measures[\"acc_te\"]), acc_measures[\"acc_te\"].shape)\n",
    "lambda_ri_jet0 = lambdas[idx[0]]\n",
    "degree_ri_jet0 = degrees[idx[1]]\n",
    "\n",
    "print(\"lambda*={lambda_}\\n\\ndegree*={degree}\\n\\nrmse train={rmse_tr}\\n\\nrmse test={rmse_te}\\n\\nacc train={acc_tr}\\n\\nacc test={acc_te}\".format(\n",
    "    lambda_=lambda_ri_jet0, degree=degree_ri_jet0, rmse_tr=rmse_tr_ri[idx], rmse_te=rmse_te_ri[idx],acc_tr=acc_measures[\"acc_tr\"][idx], acc_te=acc_measures[\"acc_te\"][idx]))\n",
    "\n",
    "vis_cv_acc(degrees,lambdas,acc_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_ri_jet0 = 11\n",
    "lambda_ri_jet0 = 1.6681005372000556e-08\n",
    "tX_jet0 = np.apply_along_axis(standardize, 1, tX_jet0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_ri, loss0_ri, acc_measures0_ri, data_meas0_ri = build_final_model(y_jet0, tX_jet0, degree_ri_jet0,\n",
    "                                                      lambda_ri_jet0, ml_function = 'ri', interaction = w_interaction)\n",
    "\n",
    "print(acc_measures0_ri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_indices = build_k_indices(y_jet1, k_fold, seed)\n",
    "\n",
    "rmse_tr_ri, rmse_te_ri, acc_measures = cross_validation_wAcc(y_jet1, tX_jet1, k_indices, k_fold, degrees, \n",
    "                                          lambdas , ml_function = 'ri', max_iters = 100, \n",
    "                                          gamma = 0.05, verbose = True, interaction=w_interaction)\n",
    "\n",
    "idx = np.unravel_index(np.nanargmax(acc_measures[\"acc_te\"]), acc_measures[\"acc_te\"].shape)\n",
    "lambda_ri_jet1 = lambdas[idx[0]]\n",
    "degree_ri_jet1 = degrees[idx[1]]\n",
    "\n",
    "print(\"lambda*={lambda_}\\n\\ndegree*={degree}\\n\\nrmse train={rmse_tr}\\n\\nrmse test={rmse_te}\\n\\nacc train={acc_tr}\\n\\nacc test={acc_te}\".format(\n",
    "    lambda_=lambda_ri_jet1, degree=degree_ri_jet1, rmse_tr=rmse_tr_ri[idx], rmse_te=rmse_te_ri[idx],acc_tr=acc_measures[\"acc_tr\"][idx], acc_te=acc_measures[\"acc_te\"][idx]))\n",
    "\n",
    "vis_cv_acc(degrees,lambdas,acc_measures)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_ri_jet1 = 8\n",
    "lambda_ri_jet1 = 4.6415888336127725e-12\n",
    "tX_jet1 = np.apply_along_axis(standardize, 1, tX_jet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_ri, loss1_ri, acc_measures1_ri, data_meas1_ri = build_final_model(y_jet1, tX_jet1, degree_ri_jet1,\n",
    "                                                      lambda_ri_jet1, ml_function = 'ri', interaction = w_interaction)\n",
    "\n",
    "print(acc_measures1_ri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_indices = build_k_indices(y_jet2, k_fold, seed)\n",
    "\n",
    "rmse_tr_ri, rmse_te_ri, acc_measures = cross_validation_wAcc(y_jet2, tX_jet2, k_indices, k_fold, degrees, \n",
    "                                          lambdas , ml_function = 'ri', max_iters = 100, \n",
    "                                          gamma = 0.05, verbose = True, interaction=w_interaction)\n",
    "\n",
    "idx = np.unravel_index(np.nanargmax(acc_measures[\"acc_te\"]), acc_measures[\"acc_te\"].shape)\n",
    "lambda_ri_jet2 = lambdas[idx[0]]\n",
    "degree_ri_jet2 = degrees[idx[1]]\n",
    "\n",
    "print(\"lambda*={lambda_}\\n\\ndegree*={degree}\\n\\nrmse train={rmse_tr}\\n\\nrmse test={rmse_te}\\n\\nacc train={acc_tr}\\n\\nacc test={acc_te}\".format(\n",
    "    lambda_=lambda_ri_jet2, degree=degree_ri_jet2, rmse_tr=rmse_tr_ri[idx], rmse_te=rmse_te_ri[idx],acc_tr=acc_measures[\"acc_tr\"][idx], acc_te=acc_measures[\"acc_te\"][idx]))\n",
    "\n",
    "vis_cv_acc(degrees,lambdas,acc_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_ri_jet2 = 11\n",
    "lambda_ri_jet2 = 2.154434690031878e-09\n",
    "tX_jet2 = np.apply_along_axis(standardize, 1, tX_jet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2_ri, loss2_ri, acc_measures2_ri, data_meas2_ri = build_final_model(y_jet2, tX_jet2, degree_ri_jet2,\n",
    "                                                      lambda_ri_jet2, ml_function = 'ri', interaction = w_interaction)\n",
    "\n",
    "print(acc_measures2_ri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltX = build_poly_inter(tX.copy(), 1, interaction = w_interaction)\n",
    "ltX, _, _ = standardize_train(ltX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yl = np.where(y==-1, 0, y)\n",
    "max_iters = 1000\n",
    "gammas = np.logspace(-10, -1, 20)\n",
    "initial_w = np.zeros(ltX.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_lr = np.empty(len(gammas))\n",
    "ws_lr = np.empty((len(gammas), len(initial_w)))\n",
    "for idx, gamma in enumerate(gammas):\n",
    "    (w, loss) = logistic_regression(yl, ltX, initial_w, max_iters, gamma)\n",
    "    losses_lr[idx] = loss\n",
    "    ws_lr[idx, :] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the losses per gamma used\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogx(gammas, losses_lr)\n",
    "\n",
    "ax.set(xlabel='gamma', ylabel='loglikelihood',\n",
    "       title='Log likelihood per choice of learning rate')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.nanargmin(losses_lr)\n",
    "\n",
    "w_lr = ws_lr[idx]\n",
    "gamma_lr = gammas[idx]\n",
    "\n",
    "print(\"w* ={w}\\n\\nloglikelihood loss={loss}\\n\\ngamma={gamma}\".format(\n",
    "    w=w_lr, loss=losses_lr[idx], gamma = gamma_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation for hyperparameter determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "k_fold = 4\n",
    "degrees = range(1, 9)\n",
    "\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "loss_tr_lr, loss_te_lr, acc_measures_lr = cross_validation_wAcc(yl, tX, k_indices, k_fold, degrees, lambdas = [0],\n",
    "                                                                 ml_function = 'lr', max_iters = 1000, gamma = gamma_lr,\n",
    "                                                                 verbose = True, interaction = w_interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_cv_acc(degrees,lambdas,acc_measures_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_te_lr = np.array(loss_te_lr)\n",
    "loss_tr_lr = np.array(loss_tr_lr)\n",
    "\n",
    "idx = np.nanargmax(acc_measures_lr[\"acc_te\"])\n",
    "\n",
    "degree_lr = degrees[idx]\n",
    "\n",
    "print(\"degree*={degree}\\n\\nloglikelihood train={loss_tr}\\n\\nloglikelihood test={loss_te}\\n\\nacc train={acc_tr}\\n\\nacc test={acc_te}\".format(\n",
    "    degree=degree_lr, loss_tr=loss_tr_lr.flatten()[idx], loss_te=loss_te_lr.flatten()[idx],\n",
    "    acc_tr=acc_measures_lr[\"acc_tr\"].flatten()[idx], acc_te=acc_measures_lr[\"acc_te\"].flatten()[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltX = build_poly_inter(tX.copy(), 1, interaction = w_interaction)\n",
    "ltX, _, _ = standardize_train(ltX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(ltX.shape[1])\n",
    "max_iters = 1000\n",
    "gammas = np.logspace(-10, -1, 20)\n",
    "lambda_rlr = lambda_ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_rlr = np.empty(len(gammas))\n",
    "ws_rlr = np.empty((len(gammas), len(initial_w)))\n",
    "for idx, gamma in enumerate(gammas):\n",
    "    (w, loss) = reg_logistic_regression(yl, ltX, lambda_rlr, initial_w, max_iters, gamma)\n",
    "    losses_rlr[idx] = loss\n",
    "    ws_rlr[idx, :] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the losses per gamma used\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogx(gammas, losses_rlr)\n",
    "\n",
    "ax.set(xlabel='gamma', ylabel='loglikelihood',\n",
    "       title='Log likelihood per choice of learning rate')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.nanargmin(losses_rlr)\n",
    "\n",
    "w_rlr = ws_rlr[idx]\n",
    "gamma_rlr = gammas[idx]\n",
    "\n",
    "print(\"w* ={w}\\n\\nloglikelihood loss={loss}\\n\\ngamma={gamma}\".format(\n",
    "    w=w_rlr, loss=losses_rlr[idx], gamma = gamma_rlr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "degree_rlr = 1\n",
    "k_fold = 4\n",
    "lambdas = np.logspace(-8, -2, 5)\n",
    "degrees = range(1, 5)\n",
    "\n",
    "k_indices = build_k_indices(yl, k_fold, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tr_rlr, loss_te_rlr, acc_measures_rlr = cross_validation_wAcc(yl, tX, k_indices, k_fold, degrees,\n",
    "                                                                   lambdas, ml_function = 'rlr', max_iters = 500,\n",
    "                                                                   gamma = gamma_rlr, verbose = True,\n",
    "                                                                   interaction = w_interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_cv_acc(degrees,lambdas, acc_measures_rlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.unravel_index(np.nanargmax(acc_measures_rlr[\"acc_te\"]), acc_measures_rlr[\"acc_te\"].shape)\n",
    "lambda_rlr = lambdas[idx[0]]\n",
    "degree_rlr = degrees[idx[1]]\n",
    "\n",
    "print(\"degree*={degree}\\n\\nloglikelihood train={loss_tr}\\n\\nloglikelihood test={loss_te}\\n\\nacc train={acc_tr}\\n\\nacc test={acc_te}\".format(\n",
    "    degree=degree_rlr, loss_tr=loss_tr_rlr[idx], loss_te=loss_te_rlr[idx],\n",
    "    acc_tr=acc_measures_rlr[\"acc_tr\"][idx], acc_te=acc_measures_rlr[\"acc_te\"][idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias-variance decomposition for complexity determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_train = 0.1\n",
    "seeds = range(50)\n",
    "degrees = range(1, 9)\n",
    "lambda_rlr = lambda_ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tr_bv = np.empty((len(seeds), len(degrees)))\n",
    "loss_te_bv = np.empty((len(seeds), len(degrees)))\n",
    "\n",
    "for index_seed, seed in enumerate(seeds):\n",
    "    np.random.seed(seed)\n",
    "        \n",
    "    x_tr, x_te, y_tr, y_te = split_data(tX, y, ratio_train, seed)  \n",
    "    \n",
    "    x_tr, x_te = standardize_both(x_tr, x_te)\n",
    "\n",
    "    for index_deg, deg in enumerate(degrees):\n",
    "        tx_tr = build_poly(x_tr, deg)\n",
    "        tx_te = build_poly(x_te, deg)\n",
    "        \n",
    "        initial_w = np.zeros(tx_tr.shape[1])\n",
    "        \n",
    "        w_tr, log_tr = reg_logistic_regression(y_tr, tx_tr, lambda_rlr, initial_w, max_iters, gamma_rlr)\n",
    "        log_te = compute_loglikelihood(y_te, tx_te, w_tr)\n",
    "            \n",
    "        loss_tr_bv[index_seed][index_deg] = log_tr\n",
    "        loss_tr_bv[index_seed][index_deg] = log_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_variance_decomposition_visualization(degrees, loss_tr_bv, loss_te_bv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plot above, change the value of `degree_rlr` manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.choice(tX_test.shape[0], 1500, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test = tX_test[indices,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = make_prediction(tx=tX_test.copy(), weights = w_ri.copy(), rmx = rmX.copy(), median = mtX.copy(),\n",
    "                          d=degree_ri, train_data_measures=data_meas_ri, \n",
    "                         interactions = w_interaction, ml_function = \"ri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv'\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Jet sub-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.zeros(tX_test.shape[0])\n",
    "idx_test0, y_test0, tX_test0, idx_test1, y_test1, tX_test1, idx_test2, y_test2, tX_test2 = separate_jet(y_test, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_test0 = np.apply_along_axis(standardize, 1, tX_test0)\n",
    "y_test0 = make_prediction(tX_test0, w0_ri, rmX_jet0, median_jet0,\n",
    "                          degree_ri_jet0, data_meas0_ri, interactions = w_interaction, ml_function = \"ri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test1 = make_prediction(tX_test1, w1_ri, rmX_jet1, median_jet1,\n",
    "                          degree_ri_jet1, data_meas1_ri, interactions = w_interaction, ml_function = \"ri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test2 = make_prediction(tX_test2, w2_ri, rmX_jet2, median_jet2,\n",
    "                          degree_ri_jet2, data_meas2_ri, interactions = w_interaction, ml_function = \"ri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission_best.csv'\n",
    "y_pred = merge_jet(idx_test0, y_test0, idx_test1, y_test1, idx_test2, y_test2)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
