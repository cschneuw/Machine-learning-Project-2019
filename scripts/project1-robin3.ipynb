{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple data description \n",
    "- all variables are floating point, except PRI_jet_num which is integer\n",
    "- variables prefixed with PRI (for PRImitives) are “raw” quantities about the bunch collision as measured by the detector.\n",
    "- variables prefixed with DER (for DERived) are quantities computed from the primitive features, which were selected by the physicists of ATLAS.\n",
    "- it can happen that for some entries some variables are meaningless or cannot be computed; in this case, their value is −999.0, which is outside the normal range of all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape = (250000,)\n",
      "tX shape =(250000, 30)\n",
      "ids shape = (250000,)\n"
     ]
    }
   ],
   "source": [
    "print('y shape = ' + str(y.shape) + '\\ntX shape =' + str(tX.shape) + '\\nids shape = ' + str(ids.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of the data:\n",
    "- `y` (N) is composed of the labels (-1 or 1) of all the samples.  \n",
    "- `tX` (N x F) is composed of the values of the features (F) for all samples (N)  \n",
    "- `ids` (N) is composed of all the index (100000-349999) of the samples (N)  \n",
    "  \n",
    "Moreover, the number of features is 30 (F=30) and the number of samples is 250'000 (N=250'000). Non recorded data has value `-999`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tX0 = np.where(tX==-999, np.nan,tX)\n",
    "\n",
    "for f in [2, 6, 10, 15, 20, 27]:\n",
    "    plot_feature(ids, tX0, y, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for the data pre-processing, the first step is to remove the visible outliers in the Exploratory Analysis plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers based on the plots in the Exploratory analysis\n",
    "y, tX = remove_outliers(y, tX, [0, 2, 3, 8, 13, 16, 19, 21, 23, 26],\n",
    "                       [1100, 1000, 1000, 2500, 500, 500, 800, 1800, 800, 600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then with this data, we separate the data points based on the categorical data: jet. We keep three matrices with `jet = 0`, `jet = 1` and `jet > 2`. We will futher train separately our models on each of these sub-matrices and have separated, learning rates, penalities or polynomial degrees for each. \n",
    "\n",
    "We still keep the whole data matrix to do the same treatments and generate whole weight matrices as a comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.choice(tX.shape[0], 15000, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y[indices,]\n",
    "tX = tX[indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx0, y_jet0, tX_jet0, idx1, y_jet1, tX_jet1, idx2, y_jet2, tX_jet2 = separate_jet(y, tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each dataset, we clean them by setting all missing values `-999` to `NaN`, then when for a feature the ratio of missing values reaches a threshold, we remove the whole feature from the dataset. The remaining missing data are replaced by the median value in the column (we also tried to replace the missing data by the mean value of the feature or a gaussian distribution. Then we standarize the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingness_cutoff = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'implementations' from '/Users/Robin/Documents/EPFL/2019-2020/ML/Machine-learning-Project-2019/scripts/implementations.py'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(implementations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX, rmX = missingness_filter(tX, missingness_cutoff)\n",
    "rmX = np.append(rmX, 22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-bc997462465d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimplementations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpute_median_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/EPFL/2019-2020/ML/Machine-learning-Project-2019/scripts/implementations.py\u001b[0m in \u001b[0;36mimpute_median_train\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimpute_median_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m     \u001b[0mmedian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m     \u001b[0minds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mnanmedian\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/numpy/lib/nanfunctions.py\u001b[0m in \u001b[0;36mnanmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n\u001b[0;32m-> 1115\u001b[0;31m                                   overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   1116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3408\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3410\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3411\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/numpy/lib/nanfunctions.py\u001b[0m in \u001b[0;36m_nanmedian\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0mpart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_nanmedian1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nanmedian1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/numpy/lib/nanfunctions.py\u001b[0m in \u001b[0;36m_nanmedian1d\u001b[0;34m(arr1d, overwrite_input)\u001b[0m\n\u001b[1;32m    965\u001b[0m     \"\"\"\n\u001b[1;32m    966\u001b[0m     arr1d, overwrite_input = _remove_nan_1d(arr1d,\n\u001b[0;32m--> 967\u001b[0;31m                                             overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m    968\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marr1d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/numpy/lib/nanfunctions.py\u001b[0m in \u001b[0;36m_remove_nan_1d\u001b[0;34m(arr1d, overwrite_input)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \"\"\"\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr1d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0marr1d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "tX, mtX = impute_median_train(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_jet0, rmX_jet0 = missingness_filter(tX_jet0, missingness_cutoff)\n",
    "tX_jet1, rmX_jet1 = missingness_filter(tX_jet1, missingness_cutoff)\n",
    "tX_jet2, rmX_jet2 = missingness_filter(tX_jet2, missingness_cutoff)\n",
    "tX_jet0, median_jet0 = impute_median_train(tX_jet0)\n",
    "tX_jet1, median_jet1 = impute_median_train(tX_jet1)\n",
    "tX_jet2, median_jet2 = impute_median_train(tX_jet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tX shape={tXs}\\ntX_jet0 shape={tX0s}\\ntX_jet1 shape={tX1s}\\ntX_jet2+ shape={tX2s}\".format(\n",
    "    tXs=tX.shape, tX0s=tX_jet0.shape, tX1s=tX_jet1.shape, tX2s=tX_jet2.shape))\n",
    "\n",
    "print(\"\\nremoved columns for :\\ntX={rmX}\\ntX_jet0={rmX0}\\ntX_jet1={rmX1}\\ntX_jet2+={rmX2}\".format(\n",
    "    rmX=rmX, rmX0=rmX_jet0, rmX1=rmX_jet1, rmX2=rmX_jet2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(w_ls, loss_ls) = least_squares(y, tX)\n",
    "\n",
    "print(\"w* ={w}\\n\\nmse={loss}\".format(w=w_ls, loss=loss_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jet sub-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for index_jet, (yi, txi) in enumerate(zip([y_jet0, y_jet1, y_jet2], [tX_jet0, tX_jet1, tX_jet2])):\n",
    "    (w_ls, loss_ls) = least_squares(yi, txi)\n",
    "    print(\"w_jet{jet}* = {w}\\nmse = {loss}\\n\\n\".format(\n",
    "    jet=index_jet, w=w_ls, loss=loss_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define parameters\n",
    "initial_w = np.zeros(tX.shape[1])\n",
    "max_iters = 100\n",
    "gammas = np.logspace(-6, 0, 20)\n",
    "\n",
    "losses_gd = np.empty(len(gammas))\n",
    "ws_gd = np.empty((len(gammas), len(initial_w)))\n",
    "\n",
    "for idx, gamma in enumerate(gammas):\n",
    "    (w, loss) = least_squares_GD(y, tX, initial_w, max_iters, gamma)\n",
    "    losses_gd[idx] = loss\n",
    "    ws_gd[idx, :]=w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the losses per gamma\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogx(gammas, losses_gd)\n",
    "\n",
    "ax.set(xlabel='gamma', ylabel='MSE',\n",
    "       title='Mean square error per choice of learning rate')\n",
    "ax.grid()\n",
    "ax.set_ylim([0, 0.8])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.nanargmin(losses_gd)\n",
    "\n",
    "w_gd = ws_gd[idx]\n",
    "gamma_gd = gammas[idx]\n",
    "\n",
    "print(\"w* ={w}\\n\\nmse={loss}\\n\\ngamma={gamma}\".format(\n",
    "    w=w_gd, loss=losses_gd[idx], gamma=gamma_gd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jet sub-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#define parameters\n",
    "max_iters = 100\n",
    "gammas = np.logspace(-6, 0, 10)\n",
    "\n",
    "for index_jet, (yi, txi) in enumerate(zip([y_jet0, y_jet1, y_jet2], [tX_jet0, tX_jet1, tX_jet2])):\n",
    "    initial_w = np.zeros(txi.shape[1])\n",
    "    losses_gd = np.empty(len(gammas))\n",
    "    ws_gd = np.empty((len(gammas), len(initial_w)))\n",
    "\n",
    "    for index_g, gamma in enumerate(gammas):\n",
    "        (w, loss) = least_squares_GD(yi, txi, initial_w, max_iters, gamma)\n",
    "        losses_gd[index_g] = loss\n",
    "        ws_gd[index_g, :]=w\n",
    "        \n",
    "    idx = np.nanargmin(losses_gd)\n",
    "    w_gd = ws_gd[idx]\n",
    "    gamma_gd = gammas[idx]\n",
    "    print(\"w_jet{jet}* = {w}\\nmse = {loss}\\ngamma = {gamma}\".format(\n",
    "    jet=index_jet, w=w_gd, loss=losses_gd[idx], gamma=gamma_gd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(tX.shape[1])\n",
    "max_iters = 100\n",
    "gammas = np.logspace(-6, 0, 20)\n",
    "\n",
    "losses_sgd = np.empty(len(gammas))\n",
    "ws_sgd = np.empty((len(gammas), len(initial_w)))\n",
    "for idx, gamma in enumerate(gammas):\n",
    "    (w, loss) = least_squares_SGD(y, tX, initial_w, max_iters, gamma)\n",
    "    losses_sgd[idx] = loss\n",
    "    ws_sgd[idx,:] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the losses per gamma used\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogx(gammas, losses_sgd)\n",
    "\n",
    "ax.set(xlabel='gamma', ylabel='MSE',\n",
    "       title='Mean square error per choice of learning rate')\n",
    "ax.grid()\n",
    "ax.set_ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.nanargmin(losses_sgd)\n",
    "\n",
    "w_sgd = ws_sgd[idx]\n",
    "gamma_sgd = gammas[idx]\n",
    "\n",
    "print(\"w*={w}\\n\\nmse={loss}\\n\\ngamma={gamma}\".format(\n",
    "    w=w_sgd, loss=losses_sgd[idx], gamma=gamma_sgd ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jet sub-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define parameters\n",
    "max_iters = 100\n",
    "gammas = np.logspace(-6, 0, 10)\n",
    "\n",
    "for index_jet, (yi, txi) in enumerate(zip([y_jet0, y_jet1, y_jet2], [tX_jet0, tX_jet1, tX_jet2])):\n",
    "    initial_w = np.zeros(txi.shape[1])\n",
    "    losses_gd = np.empty(len(gammas))\n",
    "    ws_gd = np.empty((len(gammas), len(initial_w)))\n",
    "\n",
    "    for index_g, gamma in enumerate(gammas):\n",
    "        (w, loss) = least_squares_SGD(yi, txi, initial_w, max_iters, gamma)\n",
    "        losses_gd[index_g] = loss\n",
    "        ws_gd[index_g, :]=w\n",
    "        \n",
    "    idx = np.nanargmin(losses_gd)\n",
    "    w_gd = ws_gd[idx]\n",
    "    gamma_gd = gammas[idx]\n",
    "    print(\"w_jet{jet}* = {w}\\nmse = {loss}\\ngamma = {gamma}\".format(\n",
    "    jet=index_jet, w=w_gd, loss=losses_gd[idx], gamma=gamma_gd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "\n",
    "For ridge regression, we have two parameters to optimize, the lambda (penality) and degree (complexity). To do so, we use a cross validation that optimizes both at the same time. \n",
    "\n",
    "Then a biais and variance decomposition is used to visualize if the methods tend to underfit or overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_cv_acc(degrees,lambdas,acc_measures):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    cross_validation_visualization(degrees, acc_measures[\"acc_tr\"], acc_measures[\"acc_te\"], lambdas, \"accuracy\")\n",
    "    plt.subplot(2, 2, 2)\n",
    "    cross_validation_visualization(degrees, acc_measures[\"pre_tr\"], acc_measures[\"pre_te\"], lambdas, \"precision\")\n",
    "    plt.subplot(2, 2, 3)\n",
    "    cross_validation_visualization(degrees, acc_measures[\"rec_tr\"], acc_measures[\"rec_te\"], lambdas, \"recall\")\n",
    "    plt.subplot(2, 2, 4)\n",
    "    cross_validation_visualization(degrees, acc_measures[\"f1_tr\"], acc_measures[\"f1_te\"], lambdas, \"F1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "degree_ri = 1\n",
    "k_fold = 4\n",
    "lambdas = np.logspace(-20, -8, 13)\n",
    "degrees = range(1, 15)\n",
    "w_interaction = True\n",
    "\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "rmse_tr_ri, rmse_te_ri, acc_measures = cross_validation_wAcc(y, tX, k_indices, k_fold, degrees, \n",
    "                                          lambdas , ml_function = 'ri', max_iters = 100, \n",
    "                                          gamma = 0.05, verbose = True, interaction=w_interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_cv_acc(degrees,lambdas,acc_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.unravel_index(np.nanargmax(acc_measures[\"acc_te\"]), acc_measures[\"acc_te\"].shape)\n",
    "lambda_ri = lambdas[idx[0]]\n",
    "degree_ri = degrees[idx[1]]\n",
    "\n",
    "print(\"lambda*={lambda_}\\n\\ndegree*={degree}\\n\\nrmse train={rmse_tr}\\n\\nrmse test={rmse_te}\\n\\nacc train={acc_tr}\\n\\nacc test={acc_te}\".format(\n",
    "    lambda_=lambda_ri, degree=degree_ri, rmse_tr=rmse_tr_ri[idx], rmse_te=rmse_te_ri[idx],acc_tr=acc_measures[\"acc_tr\"][idx], acc_te=acc_measures[\"acc_te\"][idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias-variance decomposition for complexity determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_train = 0.1\n",
    "seeds = range(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_tr_bv = np.empty((len(seeds), len(degrees)))\n",
    "rmse_te_bv = np.empty((len(seeds), len(degrees)))\n",
    "\n",
    "for index_seed, seed in enumerate(seeds):\n",
    "    np.random.seed(seed)\n",
    "    x_tr, x_te, y_tr, y_te = split_data(tX, y, ratio_train, seed)        \n",
    "    \n",
    "    for index_deg, deg in enumerate(degrees): \n",
    "        tx_tr = build_poly(x_tr, deg)\n",
    "        tx_te = build_poly(x_te, deg)\n",
    "            \n",
    "        w_tr, mse_tr = ridge_regression(y_tr, tx_tr, lambda_ri)\n",
    "        mse_te = compute_mse(y_te, tx_te, w_tr)\n",
    "            \n",
    "        rmse_tr_bv[index_seed][index_deg] = np.sqrt(2*mse_tr)\n",
    "        rmse_te_bv[index_seed][index_deg] = np.sqrt(2*mse_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_variance_decomposition_visualization(degrees, rmse_tr_bv, rmse_te_bv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plot above, change the value of `degree_ri` manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_poly = build_poly(tX, degree_ri)\n",
    "            \n",
    "w_ri, mse_ri = ridge_regression(y, tX_poly, lambda_ri)\n",
    "\n",
    "print(\"final w* shape={w}\\n\\nfinal degree*={degree}\\n\\nfinal lambda*={lambda_}\\n\\nmse={loss}\".format(w=w_ri.shape, \\\n",
    "            degree=degree_ri, lambda_=lambda_ri, loss = mse_ri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jet sub-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation for hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "degree_ri = 1\n",
    "k_fold = 4\n",
    "lambdas = np.logspace(-20, -8, 13)\n",
    "degrees = range(1, 15)\n",
    "\n",
    "k_indices = build_k_indices(y_jet0, k_fold, seed)\n",
    "\n",
    "rmse_tr_ri, rmse_te_ri, acc_measures = cross_validation_wAcc(y_jet0, tX_jet0, k_indices, k_fold, degrees, \n",
    "                                          lambdas , ml_function = 'ri', max_iters = 100, \n",
    "                                          gamma = 0.05, verbose = True, interaction=w_interaction)\n",
    "\n",
    "idx = np.unravel_index(np.nanargmax(acc_measures[\"acc_te\"]), acc_measures[\"acc_te\"].shape)\n",
    "lambda_ri = lambdas[idx[0]]\n",
    "degree_ri = degrees[idx[1]]\n",
    "\n",
    "print(\"lambda*={lambda_}\\n\\ndegree*={degree}\\n\\nrmse train={rmse_tr}\\n\\nrmse test={rmse_te}\\n\\nacc train={acc_tr}\\n\\nacc test={acc_te}\".format(\n",
    "    lambda_=lambda_ri, degree=degree_ri, rmse_tr=rmse_tr_ri[idx], rmse_te=rmse_te_ri[idx],acc_tr=acc_measures[\"acc_tr\"][idx], acc_te=acc_measures[\"acc_te\"][idx]))\n",
    "\n",
    "vis_cv_acc(degrees,lambdas,acc_measures)\n",
    "\n",
    "w0_ri, loss0_ri, acc_measures0_ri, data_meas0_ri = build_final_model(y_jet0, tX_jet0, degree_ri,\n",
    "                                                      lambda_ri, ml_function = 'ri', interaction = w_interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_indices = build_k_indices(y_jet1, k_fold, seed)\n",
    "\n",
    "rmse_tr_ri, rmse_te_ri, acc_measures = cross_validation_wAcc(y_jet1, tX_jet1, k_indices, k_fold, degrees, \n",
    "                                          lambdas , ml_function = 'ri', max_iters = 100, \n",
    "                                          gamma = 0.05, verbose = True, interaction=w_interaction)\n",
    "\n",
    "idx = np.unravel_index(np.nanargmax(acc_measures[\"acc_te\"]), acc_measures[\"acc_te\"].shape)\n",
    "lambda_ri = lambdas[idx[0]]\n",
    "degree_ri = degrees[idx[1]]\n",
    "\n",
    "print(\"lambda*={lambda_}\\n\\ndegree*={degree}\\n\\nrmse train={rmse_tr}\\n\\nrmse test={rmse_te}\\n\\nacc train={acc_tr}\\n\\nacc test={acc_te}\".format(\n",
    "    lambda_=lambda_ri, degree=degree_ri, rmse_tr=rmse_tr_ri[idx], rmse_te=rmse_te_ri[idx],acc_tr=acc_measures[\"acc_tr\"][idx], acc_te=acc_measures[\"acc_te\"][idx]))\n",
    "\n",
    "vis_cv_acc(degrees,lambdas,acc_measures)\n",
    "\n",
    "w1_ri, loss1_ri, acc_measures1_ri, data_meas1_ri = build_final_model(y_jet1, tX_jet1, degree_ri,\n",
    "                                                      lambda_ri, ml_function = 'ri', interaction = w_interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_indices = build_k_indices(y_jet2, k_fold, seed)\n",
    "\n",
    "rmse_tr_ri, rmse_te_ri, acc_measures = cross_validation_wAcc(y_jet2, tX_jet2, k_indices, k_fold, degrees, \n",
    "                                          lambdas , ml_function = 'ri', max_iters = 100, \n",
    "                                          gamma = 0.05, verbose = True, interaction=w_interaction)\n",
    "\n",
    "idx = np.unravel_index(np.nanargmax(acc_measures[\"acc_te\"]), acc_measures[\"acc_te\"].shape)\n",
    "lambda_ri = lambdas[idx[0]]\n",
    "degree_ri = degrees[idx[1]]\n",
    "\n",
    "print(\"lambda*={lambda_}\\n\\ndegree*={degree}\\n\\nrmse train={rmse_tr}\\n\\nrmse test={rmse_te}\\n\\nacc train={acc_tr}\\n\\nacc test={acc_te}\".format(\n",
    "    lambda_=lambda_ri, degree=degree_ri, rmse_tr=rmse_tr_ri[idx], rmse_te=rmse_te_ri[idx],acc_tr=acc_measures[\"acc_tr\"][idx], acc_te=acc_measures[\"acc_te\"][idx]))\n",
    "\n",
    "vis_cv_acc(degrees,lambdas,acc_measures)\n",
    "\n",
    "w2_ri, loss2_ri, acc_measures2_ri, data_meas2_ri = build_final_model(y_jet2, tX_jet2, degree_ri,\n",
    "                                                      lambda_ri, ml_function = 'ri', interaction = w_interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltX = build_poly_inter(tX, 1, interaction = w_interaction)\n",
    "ltX, _, _ = standardize_train(ltX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yl = np.where(y==-1, 0, y)\n",
    "max_iters = 1000\n",
    "gammas = np.logspace(-10, -1, 20)\n",
    "initial_w = np.zeros(ltX.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_lr = np.empty(len(gammas))\n",
    "ws_lr = np.empty((len(gammas), len(initial_w)))\n",
    "for idx, gamma in enumerate(gammas):\n",
    "    (w, loss) = logistic_regression(yl, ltX, initial_w, max_iters, gamma)\n",
    "    losses_lr[idx] = loss\n",
    "    ws_lr[idx, :] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the losses per gamma used\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogx(gammas, losses_lr)\n",
    "\n",
    "ax.set(xlabel='gamma', ylabel='loglikelihood',\n",
    "       title='Log likelihood per choice of learning rate')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.nanargmin(losses_lr)\n",
    "\n",
    "w_lr = ws_lr[idx]\n",
    "gamma_lr = gammas[idx]\n",
    "\n",
    "print(\"w* ={w}\\n\\nloglikelihood loss={loss}\\n\\ngamma={gamma}\".format(\n",
    "    w=w_lr, loss=losses_lr[idx], gamma = gamma_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation for hyperparameter determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "k_fold = 4\n",
    "degrees = range(1, 9)\n",
    "\n",
    "k_indices = build_k_indices(y, k_fold, seed)\n",
    "\n",
    "loss_tr_lr, loss_te_lr, acc_measures_lr = cross_validation_wAcc(yl, tX, k_indices, k_fold, degrees, lambdas = [0],\n",
    "                                                                 ml_function = 'lr', max_iters = 1000, gamma = gamma_lr,\n",
    "                                                                 verbose = True, interaction = w_interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_cv_acc(degrees,lambdas,acc_measures_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_te_lr = np.array(loss_te_lr)\n",
    "loss_tr_lr = np.array(loss_tr_lr)\n",
    "\n",
    "idx = np.nanargmax(acc_measures_lr[\"acc_te\"])\n",
    "\n",
    "degree_lr = degrees[idx]\n",
    "\n",
    "print(\"degree*={degree}\\n\\nloglikelihood train={loss_tr}\\n\\nloglikelihood test={loss_te}\\n\\nacc train={acc_tr}\\n\\nacc test={acc_te}\".format(\n",
    "    degree=degree_lr, loss_tr=loss_tr_lr.flatten()[idx], loss_te=loss_te_lr.flatten()[idx],\n",
    "    acc_tr=acc_measures_lr[\"acc_tr\"].flatten()[idx], acc_te=acc_measures_lr[\"acc_te\"].flatten()[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(ltX.shape[1])\n",
    "max_iters = 1000\n",
    "gammas = np.logspace(-10, -1, 20)\n",
    "lambda_rlr = lambda_ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_rlr = np.empty(len(gammas))\n",
    "ws_rlr = np.empty((len(gammas), len(initial_w)))\n",
    "for idx, gamma in enumerate(gammas):\n",
    "    (w, loss) = reg_logistic_regression(yl, ltX, lambda_rlr, initial_w, max_iters, gamma)\n",
    "    losses_rlr[idx] = loss\n",
    "    ws_rlr[idx, :] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the losses per gamma used\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogx(gammas, losses_rlr)\n",
    "\n",
    "ax.set(xlabel='gamma', ylabel='loglikelihood',\n",
    "       title='Log likelihood per choice of learning rate')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.nanargmin(losses_rlr)\n",
    "\n",
    "w_rlr = ws_rlr[idx]\n",
    "gamma_rlr = gammas[idx]\n",
    "\n",
    "print(\"w* ={w}\\n\\nloglikelihood loss={loss}\\n\\ngamma={gamma}\".format(\n",
    "    w=w_rlr, loss=losses_rlr[idx], gamma = gamma_rlr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "degree_rlr = 1\n",
    "k_fold = 4\n",
    "lambdas = np.logspace(-8, -2, 7)\n",
    "degrees = range(3, 12)\n",
    "\n",
    "k_indices = build_k_indices(yl, k_fold, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tr_rlr, loss_te_rlr, acc_measures_rlr = cross_validation_wAcc(yl, tX, k_indices, k_fold, degrees,\n",
    "                                                                   lambdas, ml_function = 'rlr', max_iters = 500,\n",
    "                                                                   gamma = gamma_rlr, verbose = True,\n",
    "                                                                   interaction = w_interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_cv_acc(degrees,lambdas, acc_measures_rlr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.unravel_index(np.nanargmax(acc_measures_rlr[\"acc_te\"]), acc_measures_rlr[\"acc_te\"].shape)\n",
    "lambda_rlr = lambdas[idx[0]]\n",
    "degree_rlr = degrees[idx[1]]\n",
    "\n",
    "print(\"degree*={degree}\\n\\nloglikelihood train={loss_tr}\\n\\nloglikelihood test={loss_te}\\n\\nacc train={acc_tr}\\n\\nacc test={acc_te}\".format(\n",
    "    degree=degree_rlr, loss_tr=loss_tr_rlr[idx], loss_te=loss_te_rlr[idx],\n",
    "    acc_tr=acc_measures_rlr[\"acc_tr\"][idx], acc_te=acc_measures_rlr[\"acc_te\"][idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias-variance decomposition for complexity determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_train = 0.1\n",
    "seeds = range(50)\n",
    "degrees = range(1, 9)\n",
    "lambda_rlr = lambda_ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tr_bv = np.empty((len(seeds), len(degrees)))\n",
    "loss_te_bv = np.empty((len(seeds), len(degrees)))\n",
    "\n",
    "for index_seed, seed in enumerate(seeds):\n",
    "    np.random.seed(seed)\n",
    "        \n",
    "    x_tr, x_te, y_tr, y_te = split_data(tX, y, ratio_train, seed)        \n",
    "\n",
    "    for index_deg, deg in enumerate(degrees):\n",
    "        tx_tr = build_poly(x_tr, deg)\n",
    "        tx_te = build_poly(x_te, deg)\n",
    "        \n",
    "        initial_w = np.zeros(tx_tr.shape[1])\n",
    "        \n",
    "        w_tr, log_tr = reg_logistic_regression(y_tr, tx_tr, lambda_rlr, initial_w, max_iters, gamma_rlr)\n",
    "        log_te = compute_loglikelihood(y_te, tx_te, w_tr)\n",
    "            \n",
    "        loss_tr_bv[index_seed][index_deg] = log_tr\n",
    "        loss_tr_bv[index_seed][index_deg] = log_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_variance_decomposition_visualization(degrees, loss_tr_bv, loss_te_bv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plot above, change the value of `degree_rlr` manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_poly = build_poly(tX, degree_rlr)\n",
    "initial_w = np.empty(tX_poly.shape[1])            \n",
    "w_rlr, log_rlr = reg_logistic_regression(y, tX_poly, lambda_rlr, initial_w, max_iters, gamma_rlr)\n",
    "\n",
    "print(\"final w* shape={w}\\n\\nfinal degree*={degree}\\n\\nfinal lambda*={lambda_}\\n\\nlog-likelihood={loss}\".format(w=w_rlr.shape, \\\n",
    "            degree=degree_rlr, lambda_=lambda_rlr, loss = log_rlr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save computed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the calculated variables to keep them without having to re-run everything\n",
    "# w_ls, w_gd, gamma_gd, w_sgd, gamma_sgd, w_ri, degree_ri, lambda_ri\n",
    "# w_lr, gamma_lr, w_rlr, gamma_rlr, lambda_rlr, degree_rlr\n",
    "ws = (w_ls, w_gd, w_sgd, w_ri, w_lr, w_rlr)\n",
    "degs = (degree_ri , degree_rlr)\n",
    "gammas = (gamma_gd, gamma_sgd, gamma_lr, gamma_rlr)\n",
    "lambdas = (lambda_ri, lambda_rlr)\n",
    "\n",
    "np.savetxt('weights.txt', ws, fmt='%s', delimiter=',', newline='\\n')\n",
    "np.savetxt('degrees.txt', degs, fmt='%s', delimiter=',', newline='\\n')\n",
    "np.savetxt('gammas.txt', gammas, fmt='%s', delimiter=',', newline='\\n')\n",
    "np.savetxt('lambdas.txt', lambdas, fmt='%s', delimiter=',', newline='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = w_ri\n",
    "#weights = np.insert(weights, rmX, 0, axis = 0)\n",
    "tX_test = np.delete(tX_test, rmX, axis=1)\n",
    "# keep only columns that do not have too much missing data\n",
    "tX_test, _ = train_data_formatting(tX_test, degree = 1, cutoff = 1.0, \n",
    "                      imputation = impute_median, interaction = False)\n",
    "tX_test = np.apply_along_axis(standardize, 1, tX_test)\n",
    "tX_poly = build_poly(tX_test, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv'\n",
    "y_pred = predict_labels(weights, tX_poly)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Jet sub-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.zeros(tX_test.shape[0])\n",
    "idx_test0, y_test0, tX_test0, idx_test1, y_test1, tX_test1, idx_test2, y_test2, tX_test2 = separate_jet(y_test, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_meas2_ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply pre-processing to dataset jet0\n",
    "\n",
    "\n",
    "\n",
    "tX_test0, _ = train_data_formatting(tX_test0, degree = 1, cutoff = 1.0, imputation = impute_median, interaction = False)\n",
    "tX_test0[np.isnan(tX_test0)] = 0\n",
    "tX_test0 = np.apply_along_axis(standardize, 1, tX_test0)\n",
    "tX_test0_poly = build_poly(tX_test0, 11)\n",
    "\n",
    "# apply pre-processing to dataset jet1\n",
    "tX_test1 = np.delete(tX_test1, 22, axis=1)\n",
    "tX_test1 = np.delete(tX_test1, rmX_jet1, axis=1)\n",
    "tX_test1, _ = train_data_formatting(tX_test1, degree = 1, cutoff = 1.0, imputation = impute_median, interaction = False)\n",
    "tX_test1 = np.apply_along_axis(standardize, 1, tX_test1)\n",
    "tX_test1_poly = build_poly(tX_test1, 8)    \n",
    "\n",
    "# apply pre-processing to dataset jet2\n",
    "tX_test2 = np.delete(tX_test2, 22, axis=1)\n",
    "tX_test2 = np.delete(tX_test2, rmX_jet2, axis=1)\n",
    "tX_test2, _ = train_data_formatting(tX_test2, degree = 1, cutoff = 1.0, imputation = impute_median, interaction = False)\n",
    "tX_test2 = np.apply_along_axis(standardize, 1, tX_test2)\n",
    "tX_test2_poly = build_poly(tX_test2, 11) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv'\n",
    "y_test0 = predict_labels(w0_ri, tX_test0_poly)\n",
    "y_test1 = predict_labels(w1_ri, tX_test1_poly)\n",
    "y_test2 = predict_labels(w2_ri, tX_test2_poly)\n",
    "y_pred = merge_jet(idx_test0, y_test0, idx_test1, y_test1, idx_test2, y_test2)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
